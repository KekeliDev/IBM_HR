{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attrition in an Organization || Why Workers Quit?\n",
    "\n",
    "Employees are the backbone of the organization. Organization's performance is heavily based on the quality of the employees. Challenges that an organization has to face due employee attrition are:\n",
    "\n",
    "1. Expensive in terms of both money and time to train new employees.\n",
    "1. Loss of experienced employees\n",
    "1. Impact in productivity\n",
    "1. Impact profit\n",
    "\n",
    "Before getting our hands dirty with the data, first step is to frame the business question. Having clarity on below questions is very crucial because the solution that is being developed will make sense only if we have well stated problem.\n",
    "\n",
    "### Business questions to brainstorm:\n",
    "1. What factors are contributing more to employee attrition?\n",
    "1. What type of measures should the company take in order to retain their employees?\n",
    "1. What business value does the model bring?\n",
    "1. Will the model save lots of money?\n",
    "1. Which business unit faces the attrition problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "pd.set_option(\"display.max_columns\", 80)\n",
    "pd.set_option(\"display.max_rows\", 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv' does not exist: b'/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-396b7047dc67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv' does not exist: b'/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis\n",
    "\n",
    "- Find patterns in data through data visualization. Reveal hidden secrets of the data through graphs, analysis and charts.\n",
    "    - Univariate analysis\n",
    "        - Continous variables : Histograms, boxplots. This gives us understanding about the central tendency and spread\n",
    "        - Categorical variable : Bar chart showing frequency in each category\n",
    "    - Bivariate analysis\n",
    "        - Continous & Continous : Scatter plots to know how continous variables interact with each other\n",
    "        - Categorical & categorical : Stacked column chart to show how the frequencies are spread between two\n",
    "        - categorical variables\n",
    "        - Categorical & Continous : Boxplots, Swamplots or even bar charts\n",
    "- Detect outliers\n",
    "- Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(f\"{column}: Number of unique values {df[column].nunique()}\")\n",
    "    print(\"==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that '`EmployeeCount`', '`Over18`', '`StandardHours`' have only one unique values and '`EmployeeNumber`' has `1470` unique values.\n",
    "This features aren't useful for us, So we are going to drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_col = []\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == object and len(df[column].unique()) <= 30:\n",
    "        object_col.append(column)\n",
    "        print(f\"{column} : {df[column].unique()}\")\n",
    "        print(df[column].value_counts())\n",
    "        print(\"====================================\")\n",
    "object_col.remove('Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(object_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label = LabelEncoder()\n",
    "df[\"Attrition\"] = label.fit_transform(df.Attrition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_col = []\n",
    "for column in df.columns:\n",
    "    if df[column].dtypes != object and df[column].nunique() < 30:\n",
    "        print(f\"{column} : {df[column].unique()}\")\n",
    "        disc_col.append(column)\n",
    "        print(\"====================================\")\n",
    "disc_col.remove('Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col = []\n",
    "for column in df.columns:\n",
    "    if df[column].dtypes != object and df[column].nunique() > 30:\n",
    "        print(f\"{column} : Minimum: {df[column].min()}, Maximum: {df[column].max()}\")\n",
    "        cont_col.append(column)\n",
    "        print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i, column in enumerate(disc_col, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    df[df[\"Attrition\"] == 0][column].hist(bins=35, color='blue', label='Attrition = NO', alpha=0.6)\n",
    "    df[df[\"Attrition\"] == 1][column].hist(bins=35, color='red', label='Attrition = YES', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.xlabel(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `EnvironmentSatisfaction`, `JobSatisfaction`, `PerformanceRating`, and `RelationshipSatisfaction` features don't have big impact on the detrmination of `Attrition` of employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i, column in enumerate(cont_col, 1):\n",
    "    plt.subplot(2, 4, i)\n",
    "    df[df[\"Attrition\"] == 0][column].hist(bins=35, color='blue', label='Attrition = NO', alpha=0.6)\n",
    "    df[df[\"Attrition\"] == 1][column].hist(bins=35, color='red', label='Attrition = YES', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.xlabel(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions:**\n",
    "\n",
    "***\n",
    "- The workers with low `JobLevel`, `MonthlyIncome`, `YearAtCompany`, and `TotalWorkingYears` are more likely to quit there jobs.\n",
    "- `BusinessTravel` : The workers who travel alot are more likely to quit then other employees.\n",
    "\n",
    "- `Department` : The worker in `Research & Development` are more likely to stay then the workers on other departement.\n",
    "\n",
    "- `EducationField` : The workers with `Human Resources` and `Technical Degree` are more likely to quit then employees from other fields of educations.\n",
    "\n",
    "- `Gender` : The `Male` are more likely to quit.\n",
    "\n",
    "- `JobRole` : The workers in `Laboratory Technician`, `Sales Representative`, and `Human Resources` are more likely to quit the workers in other positions.\n",
    "\n",
    "- `MaritalStatus` : The workers who have `Single` marital status are more likely to quit the `Married`, and `Divorced`.\n",
    "\n",
    "- `OverTime` : The workers who work more hours are likely to quit then others.\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df.corr().nlargest(20, \"Attrition\").Attrition.index\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(df[col].corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Attrition', axis=1).corrwith(df.Attrition).plot(kind='barh', figsize=(10, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of correlation results (sample analysis):**\n",
    "- Monthly income is highly correlated with Job level.\n",
    "- Job level is highly correlated with total working hours.\n",
    "- Monthly income is highly correlated with total working hours.\n",
    "- Age is also positively correlated with the Total working hours.\n",
    "- Marital status and stock option level are negatively correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical data into dummies\n",
    "dummy_col = [column for column in df.drop('Attrition', axis=1).columns if df[column].nunique() < 20]\n",
    "data = pd.get_dummies(df, columns=dummy_col, drop_first=True, dtype='uint8')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "# Remove duplicate Features\n",
    "data = data.T.drop_duplicates()\n",
    "data = data.T\n",
    "\n",
    "# Remove Duplicate Rows\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values().plot(kind='barh', figsize=(10, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlation = data.drop('Attrition', axis=1).corrwith(data.Attrition).sort_values()\n",
    "model_col = feature_correlation[np.abs(feature_correlation) > 0.02].index\n",
    "len(model_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Applying machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data.drop('Attrition', axis=1)\n",
    "y = data.Attrition\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "X_std = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imp(df, model):\n",
    "    fi = pd.DataFrame()\n",
    "    fi[\"feature\"] = df.columns\n",
    "    fi[\"importance\"] = model.feature_importances_\n",
    "    return fi.sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What defines success?\n",
    "We have an imbalanced data, so if we predict that all our employees will stay we'll have an accuracy of `83.90%`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()[0] / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stay = (y_train.value_counts()[0] / y_train.shape)[0]\n",
    "leave = (y_train.value_counts()[1] / y_train.shape)[0]\n",
    "\n",
    "print(\"===============TRAIN=================\")\n",
    "print(f\"Staying Rate: {stay * 100:.2f}%\")\n",
    "print(f\"Leaving Rate: {leave * 100 :.2f}%\")\n",
    "\n",
    "stay = (y_test.value_counts()[0] / y_test.shape)[0]\n",
    "leave = (y_test.value_counts()[1] / y_test.shape)[0]\n",
    "\n",
    "print(\"===============TEST=================\")\n",
    "print(f\"Staying Rate: {stay * 100:.2f}%\")\n",
    "print(f\"Leaving Rate: {leave * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    if train:\n",
    "        pred = clf.predict(X_train)\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(\"Classification Report:\", end='')\n",
    "        print(f\"\\tPrecision Score: {precision_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tRecall Score: {recall_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tF1 score: {f1_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        pred = clf.predict(X_test)\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(\"Classification Report:\", end='')\n",
    "        print(f\"\\tPrecision Score: {precision_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tRecall Score: {recall_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tF1 score: {f1_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_classifier = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "lr_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "print_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=True)\n",
    "print_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rand_forest = RandomForestClassifier(n_estimators=1200, \n",
    "#                                      bootstrap=False,\n",
    "#                                      class_weight={0:stay, 1:leave}\n",
    "                                    )\n",
    "rand_forest.fit(X_train, y_train)\n",
    "\n",
    "print_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(rand_forest, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_imp(X, rand_forest)[:40]\n",
    "df.set_index('feature', inplace=True)\n",
    "df.plot(kind='barh', figsize=(10, 10))\n",
    "plt.title('Feature Importance according to Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_std, y_train)\n",
    "\n",
    "print_score(svc, X_train_std, y_train, X_test_std, y_test, train=True)\n",
    "print_score(svc, X_train_std, y_train, X_test_std, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 4. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_imp(X, xgb_clf)[:35]\n",
    "df.set_index('feature', inplace=True)\n",
    "df.plot(kind='barh', figsize=(10, 8))\n",
    "plt.title('Feature Importance according to XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 5. Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = np.array(X_train_std)\n",
    "X_test_std = np.array(X_test_std)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score_ann(label, prediction, train=True):\n",
    "    if train:\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(\"Classification Report:\", end='')\n",
    "        print(f\"\\tPrecision Score: {precision_score(label, prediction) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tRecall Score: {recall_score(label, prediction) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tF1 score: {f1_score(label, prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, prediction)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(label, prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(\"Classification Report:\", end='')\n",
    "        print(f\"\\tPrecision Score: {precision_score(label, prediction) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tRecall Score: {recall_score(label, prediction) * 100:.2f}%\")\n",
    "        print(f\"\\t\\t\\tF1 score: {f1_score(label, prediction) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(label, prediction)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "r = model.fit(X_train_std, y_train, \n",
    "              validation_data=(X_test_std, y_test), \n",
    "              epochs=50, \n",
    "              class_weight={0:1, 1:2}\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = model.evaluate(X_train_std, y_train)\n",
    "testing_score = model.evaluate(X_test_std, y_test)\n",
    "\n",
    "print(f\"TRAINING SCORE: {training_score}\")\n",
    "print(f\"TESTING SCORE: {testing_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(r.history['loss'], label='Loss')\n",
    "plt.plot(r.history['val_loss'], label='val_Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train_std)\n",
    "y_test_pred = model.predict(X_test_std)\n",
    "\n",
    "print_score_ann(y_train, y_train_pred.round(), train=True)\n",
    "print_score_ann(y_test, y_test_pred.round(), train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Attrition', axis=1)\n",
    "y = data.Attrition\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Concatinating X_train and y_train\n",
    "df = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "minority_class = df[df.Attrition == 1]\n",
    "majority_class = df[df.Attrition == 0]\n",
    "\n",
    "majority_downsample = resample(majority_class, replace=False, \n",
    "                               n_samples=minority_class.shape[0], \n",
    "                               random_state=42)\n",
    "\n",
    "df_2 = pd.concat([majority_downsample, minority_class])\n",
    "df_2.Attrition.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_2.drop('Attrition', axis=1)\n",
    "y_train = df_2.Attrition\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "X_std = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "lr_classifier.fit(X_train_std, y_train)\n",
    "\n",
    "print_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=True)\n",
    "print_score(lr_classifier, X_train_std, y_train, X_test_std, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forest = RandomForestClassifier(n_estimators=1500, \n",
    "                                     bootstrap=True, \n",
    "                                     oob_score=True\n",
    "                                    )\n",
    "rand_forest.fit(X_train, y_train)\n",
    "\n",
    "print_score(rand_forest, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(rand_forest, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_std, y_train)\n",
    "\n",
    "print_score(svc, X_train_std, y_train, X_test_std, y_test, train=True)\n",
    "print_score(svc, X_train_std, y_train, X_test_std, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 4. XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
